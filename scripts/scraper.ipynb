{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-82.5018903 35.5933928</td>\n",
       "      <td>Blue Ridge</td>\n",
       "      <td>https://beatinpaths.com/2023/06/27/blue-ridge/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-82.542723 35.6339975</td>\n",
       "      <td>Electric Stitches</td>\n",
       "      <td>https://beatinpaths.com/2023/06/26/electric-st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-97.3846095 32.905438</td>\n",
       "      <td>Big as Texas</td>\n",
       "      <td>https://beatinpaths.com/2023/03/01/big-as-texas/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.4255642 37.757802</td>\n",
       "      <td>Lacuna</td>\n",
       "      <td>https://beatinpaths.com/2023/02/12/lacuna/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.3209238 37.800035</td>\n",
       "      <td>grey rainy day in the bay</td>\n",
       "      <td>https://beatinpaths.com/2023/02/12/gray-rainy-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-118.3193304 34.0887869</td>\n",
       "      <td>Liquefaction</td>\n",
       "      <td>https://beatinpaths.com/2022/11/08/liquefaction/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-116.6790332 33.8144981</td>\n",
       "      <td>san jacinto</td>\n",
       "      <td>https://beatinpaths.com/2022/11/08/san-jacinto/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-103.5375052 47.1057161</td>\n",
       "      <td>north dakota badlands</td>\n",
       "      <td>https://beatinpaths.com/2022/10/16/north-dakot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-110.9081926 33.0589461</td>\n",
       "      <td>A Star is Always</td>\n",
       "      <td>https://beatinpaths.com/2022/09/28/a-star-is-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-106.3537347 39.1550467</td>\n",
       "      <td>The Clouds and the Mountains</td>\n",
       "      <td>https://beatinpaths.com/2022/08/03/the-clouds-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-106.6933176 40.5574621</td>\n",
       "      <td>Leaving Rainbow</td>\n",
       "      <td>https://beatinpaths.com/2022/08/03/leaving-rai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-89.1598805 36.9832341</td>\n",
       "      <td>cairo confluence</td>\n",
       "      <td>https://beatinpaths.com/2022/08/03/cairo-confl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-118.6685285 36.5219855</td>\n",
       "      <td>The Sequoias Speak</td>\n",
       "      <td>https://beatinpaths.com/2022/08/02/the-sequoia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-66.1140937 18.4637071</td>\n",
       "      <td>Isla del Encanto</td>\n",
       "      <td>https://beatinpaths.com/2022/06/07/isla-del-en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-112.9608976 37.251411</td>\n",
       "      <td>Zion</td>\n",
       "      <td>https://beatinpaths.com/2022/01/13/zion-unfini...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   location                         title  \\\n",
       "0    -82.5018903 35.5933928                    Blue Ridge   \n",
       "1     -82.542723 35.6339975             Electric Stitches   \n",
       "2     -97.3846095 32.905438                  Big as Texas   \n",
       "3    -122.4255642 37.757802                        Lacuna   \n",
       "4    -122.3209238 37.800035     grey rainy day in the bay   \n",
       "5   -118.3193304 34.0887869                  Liquefaction   \n",
       "6   -116.6790332 33.8144981                   san jacinto   \n",
       "7   -103.5375052 47.1057161         north dakota badlands   \n",
       "8   -110.9081926 33.0589461              A Star is Always   \n",
       "9   -106.3537347 39.1550467  The Clouds and the Mountains   \n",
       "10  -106.6933176 40.5574621               Leaving Rainbow   \n",
       "11   -89.1598805 36.9832341              cairo confluence   \n",
       "12  -118.6685285 36.5219855            The Sequoias Speak   \n",
       "13   -66.1140937 18.4637071              Isla del Encanto   \n",
       "14   -112.9608976 37.251411                          Zion   \n",
       "\n",
       "                                                  url  \n",
       "0      https://beatinpaths.com/2023/06/27/blue-ridge/  \n",
       "1   https://beatinpaths.com/2023/06/26/electric-st...  \n",
       "2    https://beatinpaths.com/2023/03/01/big-as-texas/  \n",
       "3          https://beatinpaths.com/2023/02/12/lacuna/  \n",
       "4   https://beatinpaths.com/2023/02/12/gray-rainy-...  \n",
       "5    https://beatinpaths.com/2022/11/08/liquefaction/  \n",
       "6     https://beatinpaths.com/2022/11/08/san-jacinto/  \n",
       "7   https://beatinpaths.com/2022/10/16/north-dakot...  \n",
       "8   https://beatinpaths.com/2022/09/28/a-star-is-a...  \n",
       "9   https://beatinpaths.com/2022/08/03/the-clouds-...  \n",
       "10  https://beatinpaths.com/2022/08/03/leaving-rai...  \n",
       "11  https://beatinpaths.com/2022/08/03/cairo-confl...  \n",
       "12  https://beatinpaths.com/2022/08/02/the-sequoia...  \n",
       "13  https://beatinpaths.com/2022/06/07/isla-del-en...  \n",
       "14  https://beatinpaths.com/2022/01/13/zion-unfini...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change poems table to the right format\n",
    "df = pd.read_csv(\"./poems_table.csv\")\n",
    "\n",
    "# Modify the 'location' column to remove the 'POINT (' and ')' parts\n",
    "df[\"location\"] = df[\"location\"].str.replace(\"POINT\", \"\")\n",
    "df[\"location\"] = df[\"location\"].str.replace(\"(\", \"\")\n",
    "df[\"location\"] = df[\"location\"].str.replace(\")\", \"\")\n",
    "\n",
    "# Write the modified data back to the CSV file\n",
    "df.to_csv(\"./poems_table.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./poems_table.csv\")\n",
    "\n",
    "# Save the DataFrame to a JSON file\n",
    "df.to_json(\"./poems_table.json\", orient=\"records\")\n",
    "\n",
    "# Convert the DataFrame to a dictionary\n",
    "data_dict = df.to_dict(\"records\")\n",
    "\n",
    "# Convert the dictionary to a JSON string without escaping the forward slash\n",
    "json_data = json.dumps(data_dict, ensure_ascii=False)\n",
    "\n",
    "# Write the JSON string to a file\n",
    "with open(\"./poems_table.json\", \"w\") as f:\n",
    "    f.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_poems(url):\n",
    "    \"\"\"\n",
    "    Scrape poem links, titles, dates, and the first line of the post from the given URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the webpage to scrape.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Pandas DataFrame containing the scraped data.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    poems = []\n",
    "    for post in soup.find_all(\"article\"):\n",
    "        title_element = post.find(\"a\", rel=\"bookmark\")\n",
    "        date_element = post.find(\"time\", class_=\"entry-date published\")\n",
    "        entry_content = post.find(\"div\", class_=\"entry-content\")\n",
    "        first_line_element = entry_content.find(\"p\") if entry_content else None\n",
    "\n",
    "        if title_element and date_element and first_line_element:\n",
    "            title = title_element.text\n",
    "            link = title_element[\"href\"]\n",
    "            date = date_element[\"datetime\"]\n",
    "            first_line = first_line_element.text.strip()\n",
    "\n",
    "            poems.append(\n",
    "                {\"title\": title, \"link\": link, \"date\": date, \"first_line\": first_line}\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(poems)\n",
    "\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = \"https://beatinpaths.com/category/poetry/\"\n",
    "\n",
    "# Scrape the poems and save to a CSV file\n",
    "df = scrape_poems(url)\n",
    "df.to_csv(\"poems.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(url):\n",
    "    \"\"\"\n",
    "    Scrape post links from the given URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the webpage to scrape.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Pandas DataFrame containing the scraped links.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    links = []\n",
    "    for post in soup.find_all(\"article\"):\n",
    "        link_element = post.find(\"a\", rel=\"bookmark\")\n",
    "\n",
    "        if link_element:\n",
    "            link = link_element[\"href\"]\n",
    "            links.append({\"link\": link})\n",
    "\n",
    "    return pd.DataFrame(links)\n",
    "\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = \"https://beatinpaths.com/category/poetry/\"\n",
    "\n",
    "# Scrape the links and save to a CSV file\n",
    "df = scrape_links(url)\n",
    "df.to_csv(\"links.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.16.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /Users/jerhadf/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.0.6)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.23.2-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Users/jerhadf/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=20.1.0 in /Users/jerhadf/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from trio~=0.17->selenium) (23.1.0)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: idna in /Users/jerhadf/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /Users/jerhadf/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Users/jerhadf/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Downloading selenium-4.16.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio-0.23.2-py3-none-any.whl (461 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m461.6/461.6 kB\u001b[0m \u001b[31m643.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: sortedcontainers, wsproto, pysocks, outcome, trio, trio-websocket, selenium\n",
      "Successfully installed outcome-1.3.0.post0 pysocks-1.7.1 selenium-4.16.0 sortedcontainers-2.4.0 trio-0.23.2 trio-websocket-0.11.1 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/jerhadf/Desktop/poetrymap/scraper.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jerhadf/Desktop/poetrymap/scraper.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mselenium\u001b[39;00m \u001b[39mimport\u001b[39;00m webdriver\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jerhadf/Desktop/poetrymap/scraper.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbs4\u001b[39;00m \u001b[39mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jerhadf/Desktop/poetrymap/scraper.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "# def scrape_poems(url, driver_path):\n",
    "#     \"\"\"\n",
    "#     Scrape poem links, titles, dates, and the first line of the post from the given URL.\n",
    "\n",
    "#     Args:\n",
    "#         url (str): URL of the webpage to scrape.\n",
    "#         driver_path (str): Path to the WebDriver executable.\n",
    "\n",
    "#     Returns:\n",
    "#         DataFrame: Pandas DataFrame containing the scraped data.\n",
    "#     \"\"\"\n",
    "#     driver = webdriver.Chrome(driver_path)\n",
    "#     driver.get(url)\n",
    "\n",
    "#     # Scroll down to the bottom of the page until no more new poems are loaded\n",
    "#     last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "#     while True:\n",
    "#         driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#         time.sleep(2)  # Wait for the new poems to load\n",
    "\n",
    "#         new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "#         if new_height == last_height:\n",
    "#             break\n",
    "#         last_height = new_height\n",
    "\n",
    "#     soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "#     poems = []\n",
    "#     for post in soup.find_all(\"article\"):\n",
    "#         title_element = post.find(\"a\", rel=\"bookmark\")\n",
    "#         date_element = post.find(\"time\", class_=\"entry-date published\")\n",
    "\n",
    "#         if title_element and date_element:\n",
    "#             title = title_element.text\n",
    "#             link = title_element[\"href\"]\n",
    "#             date = date_element[\"datetime\"]\n",
    "\n",
    "#             # Navigate to the poem page and scrape the first line\n",
    "#             driver.get(link)\n",
    "#             time.sleep(2)  # Wait for the poem page to load\n",
    "#             poem_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "#             first_line_element = poem_soup.find(\"div\", class_=\"entry-content\").find(\"p\")\n",
    "#             first_line = first_line_element.text.strip() if first_line_element else None\n",
    "\n",
    "#             poems.append(\n",
    "#                 {\"title\": title, \"link\": link, \"date\": date, \"first_line\": first_line}\n",
    "#             )\n",
    "\n",
    "#             # Navigate back to the main page\n",
    "#             driver.back()\n",
    "#             time.sleep(2)  # Wait for the main page to load\n",
    "\n",
    "#     driver.quit()\n",
    "\n",
    "#     return pd.DataFrame(poems)\n",
    "\n",
    "\n",
    "# # Path to the WebDriver executable\n",
    "# driver_path = \"/path/to/your/driver\"\n",
    "\n",
    "# # URL of the webpage to scrape\n",
    "# url = \"https://beatinpaths.com/category/poetry/\"\n",
    "\n",
    "# # Scrape the poems and save to a CSV file\n",
    "# df = scrape_poems(url, driver_path)\n",
    "# df.to_csv(\"poems.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
