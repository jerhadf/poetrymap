{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_poems(url):\n",
    "    \"\"\"\n",
    "    Scrape poem links, titles, dates, and the first line of the post from the given URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the webpage to scrape.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Pandas DataFrame containing the scraped data.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    poems = []\n",
    "    for post in soup.find_all(\"article\"):\n",
    "        title_element = post.find(\"a\", rel=\"bookmark\")\n",
    "        date_element = post.find(\"time\", class_=\"entry-date published\")\n",
    "        entry_content = post.find(\"div\", class_=\"entry-content\")\n",
    "        first_line_element = entry_content.find(\"p\") if entry_content else None\n",
    "\n",
    "        if title_element and date_element and first_line_element:\n",
    "            title = title_element.text\n",
    "            link = title_element[\"href\"]\n",
    "            date = date_element[\"datetime\"]\n",
    "            first_line = first_line_element.text.strip()\n",
    "\n",
    "            poems.append(\n",
    "                {\"title\": title, \"link\": link, \"date\": date, \"first_line\": first_line}\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(poems)\n",
    "\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = \"https://beatinpaths.com/category/poetry/\"\n",
    "\n",
    "# Scrape the poems and save to a CSV file\n",
    "df = scrape_poems(url)\n",
    "df.to_csv(\"poems.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(url):\n",
    "    \"\"\"\n",
    "    Scrape post links from the given URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the webpage to scrape.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Pandas DataFrame containing the scraped links.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    links = []\n",
    "    for post in soup.find_all('article'):\n",
    "        link_element = post.find('a', rel='bookmark')\n",
    "\n",
    "        if link_element:\n",
    "            link = link_element['href']\n",
    "            links.append({'link': link})\n",
    "\n",
    "    return pd.DataFrame(links)\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'https://beatinpaths.com/category/poetry/'\n",
    "\n",
    "# Scrape the links and save to a CSV file\n",
    "df = scrape_links(url)\n",
    "df.to_csv('links.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.16.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /Users/jerhadf/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.0.6)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.23.2-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Users/jerhadf/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=20.1.0 in /Users/jerhadf/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from trio~=0.17->selenium) (23.1.0)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: idna in /Users/jerhadf/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /Users/jerhadf/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Users/jerhadf/.pyenv/versions/3.12.0/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Downloading selenium-4.16.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio-0.23.2-py3-none-any.whl (461 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m461.6/461.6 kB\u001b[0m \u001b[31m643.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: sortedcontainers, wsproto, pysocks, outcome, trio, trio-websocket, selenium\n",
      "Successfully installed outcome-1.3.0.post0 pysocks-1.7.1 selenium-4.16.0 sortedcontainers-2.4.0 trio-0.23.2 trio-websocket-0.11.1 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/jerhadf/Desktop/poetrymap/scraper.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jerhadf/Desktop/poetrymap/scraper.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mselenium\u001b[39;00m \u001b[39mimport\u001b[39;00m webdriver\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jerhadf/Desktop/poetrymap/scraper.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbs4\u001b[39;00m \u001b[39mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jerhadf/Desktop/poetrymap/scraper.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def scrape_poems(url, driver_path):\n",
    "    \"\"\"\n",
    "    Scrape poem links, titles, dates, and the first line of the post from the given URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the webpage to scrape.\n",
    "        driver_path (str): Path to the WebDriver executable.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Pandas DataFrame containing the scraped data.\n",
    "    \"\"\"\n",
    "    driver = webdriver.Chrome(driver_path)\n",
    "    driver.get(url)\n",
    "\n",
    "    # Scroll down to the bottom of the page until no more new poems are loaded\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Wait for the new poems to load\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    poems = []\n",
    "    for post in soup.find_all('article'):\n",
    "        title_element = post.find('a', rel='bookmark')\n",
    "        date_element = post.find('time', class_='entry-date published')\n",
    "\n",
    "        if title_element and date_element:\n",
    "            title = title_element.text\n",
    "            link = title_element['href']\n",
    "            date = date_element['datetime']\n",
    "\n",
    "            # Navigate to the poem page and scrape the first line\n",
    "            driver.get(link)\n",
    "            time.sleep(2)  # Wait for the poem page to load\n",
    "            poem_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            first_line_element = poem_soup.find('div', class_='entry-content').find('p')\n",
    "            first_line = first_line_element.text.strip() if first_line_element else None\n",
    "\n",
    "            poems.append({'title': title, 'link': link, 'date': date, 'first_line': first_line})\n",
    "\n",
    "            # Navigate back to the main page\n",
    "            driver.back()\n",
    "            time.sleep(2)  # Wait for the main page to load\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return pd.DataFrame(poems)\n",
    "\n",
    "# Path to the WebDriver executable\n",
    "driver_path = '/path/to/your/driver'\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'https://beatinpaths.com/category/poetry/'\n",
    "\n",
    "# Scrape the poems and save to a CSV file\n",
    "df = scrape_poems(url, driver_path)\n",
    "df.to_csv('poems.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
